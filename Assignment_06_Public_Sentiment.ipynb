{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f805894",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "277cbb47",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('judge-1377884607_tweet_product_company.csv',encoding = \"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a9a3759",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>emotion_in_tweet_is_directed_at</th>\n",
       "      <th>is_there_an_emotion_directed_at_a_brand_or_product</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>.@wesley83 I have a 3G iPhone. After 3 hrs tweeting at #RISE_Austin, it was dead!  I need to upgrade. Plugin stations at #SXSW.</td>\n",
       "      <td>iPhone</td>\n",
       "      <td>Negative emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@jessedee Know about @fludapp ? Awesome iPad/iPhone app that you'll likely appreciate for its design. Also, they're giving free Ts at #SXSW</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@swonderlin Can not wait for #iPad 2 also. They should sale them down at #SXSW.</td>\n",
       "      <td>iPad</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@sxsw I hope this year's festival isn't as crashy as this year's iPhone app. #sxsw</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Negative emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@sxtxstate great stuff on Fri #SXSW: Marissa Mayer (Google), Tim O'Reilly (tech books/conferences) &amp;amp; Matt Mullenweg (Wordpress)</td>\n",
       "      <td>Google</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                    tweet_text  \\\n",
       "0              .@wesley83 I have a 3G iPhone. After 3 hrs tweeting at #RISE_Austin, it was dead!  I need to upgrade. Plugin stations at #SXSW.   \n",
       "1  @jessedee Know about @fludapp ? Awesome iPad/iPhone app that you'll likely appreciate for its design. Also, they're giving free Ts at #SXSW   \n",
       "2                                                              @swonderlin Can not wait for #iPad 2 also. They should sale them down at #SXSW.   \n",
       "3                                                           @sxsw I hope this year's festival isn't as crashy as this year's iPhone app. #sxsw   \n",
       "4          @sxtxstate great stuff on Fri #SXSW: Marissa Mayer (Google), Tim O'Reilly (tech books/conferences) &amp; Matt Mullenweg (Wordpress)   \n",
       "\n",
       "  emotion_in_tweet_is_directed_at  \\\n",
       "0                          iPhone   \n",
       "1              iPad or iPhone App   \n",
       "2                            iPad   \n",
       "3              iPad or iPhone App   \n",
       "4                          Google   \n",
       "\n",
       "  is_there_an_emotion_directed_at_a_brand_or_product  \n",
       "0                                   Negative emotion  \n",
       "1                                   Positive emotion  \n",
       "2                                   Positive emotion  \n",
       "3                                   Negative emotion  \n",
       "4                                   Positive emotion  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set the option to display the full text content of DataFrame columns\n",
    "\n",
    "pd.set_option('display.max_colwidth',None)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e553660",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(\"emotion_in_tweet_is_directed_at\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee116fcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>is_there_an_emotion_directed_at_a_brand_or_product</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>.@wesley83 I have a 3G iPhone. After 3 hrs tweeting at #RISE_Austin, it was dead!  I need to upgrade. Plugin stations at #SXSW.</td>\n",
       "      <td>Negative emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@jessedee Know about @fludapp ? Awesome iPad/iPhone app that you'll likely appreciate for its design. Also, they're giving free Ts at #SXSW</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@swonderlin Can not wait for #iPad 2 also. They should sale them down at #SXSW.</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@sxsw I hope this year's festival isn't as crashy as this year's iPhone app. #sxsw</td>\n",
       "      <td>Negative emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@sxtxstate great stuff on Fri #SXSW: Marissa Mayer (Google), Tim O'Reilly (tech books/conferences) &amp;amp; Matt Mullenweg (Wordpress)</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                    tweet_text  \\\n",
       "0              .@wesley83 I have a 3G iPhone. After 3 hrs tweeting at #RISE_Austin, it was dead!  I need to upgrade. Plugin stations at #SXSW.   \n",
       "1  @jessedee Know about @fludapp ? Awesome iPad/iPhone app that you'll likely appreciate for its design. Also, they're giving free Ts at #SXSW   \n",
       "2                                                              @swonderlin Can not wait for #iPad 2 also. They should sale them down at #SXSW.   \n",
       "3                                                           @sxsw I hope this year's festival isn't as crashy as this year's iPhone app. #sxsw   \n",
       "4          @sxtxstate great stuff on Fri #SXSW: Marissa Mayer (Google), Tim O'Reilly (tech books/conferences) &amp; Matt Mullenweg (Wordpress)   \n",
       "\n",
       "  is_there_an_emotion_directed_at_a_brand_or_product  \n",
       "0                                   Negative emotion  \n",
       "1                                   Positive emotion  \n",
       "2                                   Positive emotion  \n",
       "3                                   Negative emotion  \n",
       "4                                   Positive emotion  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c1c48ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming columns \n",
    "\n",
    "df.columns = ['tweet', 'sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5c280258",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>.@wesley83 I have a 3G iPhone. After 3 hrs tweeting at #RISE_Austin, it was dead!  I need to upgrade. Plugin stations at #SXSW.</td>\n",
       "      <td>Negative emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@jessedee Know about @fludapp ? Awesome iPad/iPhone app that you'll likely appreciate for its design. Also, they're giving free Ts at #SXSW</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@swonderlin Can not wait for #iPad 2 also. They should sale them down at #SXSW.</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@sxsw I hope this year's festival isn't as crashy as this year's iPhone app. #sxsw</td>\n",
       "      <td>Negative emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@sxtxstate great stuff on Fri #SXSW: Marissa Mayer (Google), Tim O'Reilly (tech books/conferences) &amp;amp; Matt Mullenweg (Wordpress)</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                         tweet  \\\n",
       "0              .@wesley83 I have a 3G iPhone. After 3 hrs tweeting at #RISE_Austin, it was dead!  I need to upgrade. Plugin stations at #SXSW.   \n",
       "1  @jessedee Know about @fludapp ? Awesome iPad/iPhone app that you'll likely appreciate for its design. Also, they're giving free Ts at #SXSW   \n",
       "2                                                              @swonderlin Can not wait for #iPad 2 also. They should sale them down at #SXSW.   \n",
       "3                                                           @sxsw I hope this year's festival isn't as crashy as this year's iPhone app. #sxsw   \n",
       "4          @sxtxstate great stuff on Fri #SXSW: Marissa Mayer (Google), Tim O'Reilly (tech books/conferences) &amp; Matt Mullenweg (Wordpress)   \n",
       "\n",
       "          sentiment  \n",
       "0  Negative emotion  \n",
       "1  Positive emotion  \n",
       "2  Positive emotion  \n",
       "3  Negative emotion  \n",
       "4  Positive emotion  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "015caaa7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9093, 2)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "545b5ebb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentiment\n",
       "No emotion toward brand or product    5389\n",
       "Positive emotion                      2978\n",
       "Negative emotion                       570\n",
       "I can't tell                           156\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dfece46a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count the number of missing values in the 'tweet' column\n",
    "\n",
    "df['tweet'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ed0a68ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# handle missing values\n",
    "\n",
    "df['tweet'].fillna('', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "89928531",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove puncuations\n",
    "\n",
    "import string\n",
    "import re\n",
    "def remove_pun(text):\n",
    "    text = ''.join([i for i in text if i not in string.punctuation])\n",
    "    # removing URL\n",
    "    text = re.sub(r'http\\S+|www\\S+|\\S+\\.com\\S+', '', text, flags=re.MULTILINE)\n",
    "    # removing the tags from the text\n",
    "    text = re.sub(r'(@\\S+) | (#\\S+)', r'', text)\n",
    "    # removing the RT from the text\n",
    "    text = re.sub(r'\\bRT\\b', r'', text)\n",
    "    # removing repeated characters\n",
    "    return re.sub(r'(.)1+', r'1', text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7196e1c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Acer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    " # Tokenize the given text into words\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "def tokenize(text):\n",
    "    words = nltk.word_tokenize(text)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "23d6a824",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Acer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Remove stopwords from the given text\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "def remove_stopwords(text):\n",
    "    output = [i for i in text if i not in stopwords]\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0284c523",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "def stemming(text):\n",
    "  stem_text = [ps.stem(word) for word in text]\n",
    "  return stem_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aa94c688",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Acer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Lemmatize each word in the given text\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "wordnet_lemm = WordNetLemmatizer()\n",
    "def lemma(text):\n",
    "    lemm_text = [wordnet_lemm.lemmatize(word) for word in text]\n",
    "    return lemm_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d750fce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(df_col):\n",
    "  corpus = []\n",
    "  for item in df_col:\n",
    "    new_item = remove_pun(item)\n",
    "    new_item = new_item.lower()\n",
    "    new_item = tokenize(new_item)\n",
    "    new_item = remove_stopwords(new_item)\n",
    "    new_item = lemma(new_item)\n",
    "    corpus.append(' '.join(str(x) for x in new_item))\n",
    "  return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "09e4d19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = preprocess(df['tweet'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4224442b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = corpus\n",
    "y = df['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d711a72b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the target variable\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(y)\n",
    "y = to_categorical(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "50b11199",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the text data using Keras Tokenizer\n",
    "\n",
    "from keras.preprocessing import text\n",
    "tokenizer = text.Tokenizer()\n",
    "tokenizer.fit_on_texts(list(corpus))\n",
    "tokenized_text = tokenizer.texts_to_sequences(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "26ecdfa3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenized_text[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "741f5cba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenized_text[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fd256f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad the tokenized_text to make all text sequences the same length (100)\n",
    "\n",
    "from keras.utils import pad_sequences\n",
    "x = pad_sequences(tokenized_text, maxlen=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1ff12134",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ba75710f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 100, 128)          1314176   \n",
      "                                                                 \n",
      " simple_rnn (SimpleRNN)      (None, 32)                5152      \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 32)                0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 4)                 132       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,319,460\n",
      "Trainable params: 1,319,460\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Define a sequential model with Embedding, SimpleRNN, Dropout, Dense, and Softmax layers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,LSTM,Embedding,SimpleRNN,Dropout\n",
    "\n",
    "# Create a Sequential model\n",
    "model = Sequential()\n",
    "\n",
    "# Add an Embedding layer\n",
    "model.add(Embedding(input_dim = len(tokenizer.word_index)+1, output_dim=128, input_length=100))\n",
    "\n",
    "# Add a SimpleRNN layer with 32 units\n",
    "model.add(SimpleRNN(32))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# Add the final Dense layer with 4 units (for 4 classes)\n",
    "model.add(Dense(4, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "87db78b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "205/205 [==============================] - 18s 74ms/step - loss: 0.9684 - accuracy: 0.5658 - val_loss: 0.8324 - val_accuracy: 0.6305\n",
      "Epoch 2/10\n",
      "205/205 [==============================] - 14s 70ms/step - loss: 0.6886 - accuracy: 0.7438 - val_loss: 0.8515 - val_accuracy: 0.6497\n",
      "Epoch 3/10\n",
      "205/205 [==============================] - 13s 63ms/step - loss: 0.4596 - accuracy: 0.8423 - val_loss: 0.9490 - val_accuracy: 0.6621\n",
      "Epoch 4/10\n",
      "205/205 [==============================] - 18s 86ms/step - loss: 0.3250 - accuracy: 0.8906 - val_loss: 1.0182 - val_accuracy: 0.6511\n",
      "Epoch 5/10\n",
      "205/205 [==============================] - 14s 69ms/step - loss: 0.2633 - accuracy: 0.9131 - val_loss: 1.0536 - val_accuracy: 0.6538\n",
      "Epoch 6/10\n",
      "205/205 [==============================] - 14s 67ms/step - loss: 0.2253 - accuracy: 0.9225 - val_loss: 1.1112 - val_accuracy: 0.6346\n",
      "Epoch 7/10\n",
      "205/205 [==============================] - 14s 67ms/step - loss: 0.1972 - accuracy: 0.9331 - val_loss: 1.1569 - val_accuracy: 0.6552\n",
      "Epoch 8/10\n",
      "205/205 [==============================] - 14s 66ms/step - loss: 0.1781 - accuracy: 0.9372 - val_loss: 1.2317 - val_accuracy: 0.6319\n",
      "Epoch 9/10\n",
      "205/205 [==============================] - 13s 65ms/step - loss: 0.1588 - accuracy: 0.9436 - val_loss: 1.2735 - val_accuracy: 0.6374\n",
      "Epoch 10/10\n",
      "205/205 [==============================] - 14s 66ms/step - loss: 0.1602 - accuracy: 0.9409 - val_loss: 1.2414 - val_accuracy: 0.6429\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1f62bd2ed70>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, epochs=10, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e1b9c4c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57/57 [==============================] - 1s 10ms/step - loss: 1.2739 - accuracy: 0.6267\n",
      "Test Accuracy: 62.67%\n"
     ]
    }
   ],
   "source": [
    "accuracy_SimpleRNN = model.evaluate(x_test, y_test)[1]\n",
    "print(f'Test Accuracy: {accuracy_SimpleRNN  * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a506afd",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "438098cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, 100, 128)          1314176   \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 64)                49408     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 4)                 260       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,363,844\n",
      "Trainable params: 1,363,844\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Create a Sequential model\n",
    "model2 = Sequential()\n",
    "\n",
    "# Add an Embedding layer\n",
    "model2.add(Embedding(input_dim = len(tokenizer.word_index)+1, output_dim=128, input_length=100))\n",
    "\n",
    "# Add a LSTM layer with 64 units\n",
    "model2.add(LSTM(64, dropout = 0.2))\n",
    "\n",
    "# Add the final Dense layer with 4 units (for 4 classes)\n",
    "model2.add(Dense(4, activation='softmax'))\n",
    "\n",
    "model2.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model2.summary() \n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8a75ee06-ef0f-4458-82d5-ea50a82ed04b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "205/205 [==============================] - 32s 129ms/step - loss: 0.9133 - accuracy: 0.6020 - val_loss: 0.8443 - val_accuracy: 0.6264\n",
      "Epoch 2/10\n",
      "205/205 [==============================] - 25s 120ms/step - loss: 0.6943 - accuracy: 0.7264 - val_loss: 0.8195 - val_accuracy: 0.6580\n",
      "Epoch 3/10\n",
      "205/205 [==============================] - 25s 123ms/step - loss: 0.4865 - accuracy: 0.8152 - val_loss: 0.8542 - val_accuracy: 0.6690\n",
      "Epoch 4/10\n",
      "205/205 [==============================] - 27s 131ms/step - loss: 0.3593 - accuracy: 0.8593 - val_loss: 1.0142 - val_accuracy: 0.6415\n",
      "Epoch 5/10\n",
      "205/205 [==============================] - 30s 144ms/step - loss: 0.2953 - accuracy: 0.8827 - val_loss: 0.9841 - val_accuracy: 0.6703\n",
      "Epoch 6/10\n",
      "205/205 [==============================] - 28s 136ms/step - loss: 0.2420 - accuracy: 0.9015 - val_loss: 1.1559 - val_accuracy: 0.6690\n",
      "Epoch 7/10\n",
      "205/205 [==============================] - 29s 142ms/step - loss: 0.2076 - accuracy: 0.9128 - val_loss: 1.2773 - val_accuracy: 0.6538\n",
      "Epoch 8/10\n",
      "205/205 [==============================] - 30s 145ms/step - loss: 0.1796 - accuracy: 0.9206 - val_loss: 1.4376 - val_accuracy: 0.6607\n",
      "Epoch 9/10\n",
      "205/205 [==============================] - 29s 140ms/step - loss: 0.1620 - accuracy: 0.9265 - val_loss: 1.4021 - val_accuracy: 0.6593\n",
      "Epoch 10/10\n",
      "205/205 [==============================] - 27s 134ms/step - loss: 0.1482 - accuracy: 0.9310 - val_loss: 1.5058 - val_accuracy: 0.6429\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1f62c08ccd0>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.fit(x_train, y_train, epochs=10, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "489b7347",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57/57 [==============================] - 2s 32ms/step - loss: 1.5703 - accuracy: 0.6278\n",
      "Test Accuracy: 62.78%\n"
     ]
    }
   ],
   "source": [
    "accuracy_LSTM = model2.evaluate(x_test, y_test)[1]\n",
    "print(f'Test Accuracy: {accuracy_LSTM * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb7a862-d34e-4c82-8046-285ba8fb1032",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
